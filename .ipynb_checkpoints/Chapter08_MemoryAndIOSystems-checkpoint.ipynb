{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afraid-scope",
   "metadata": {},
   "source": [
    "## Chapter 8 - Microarchitecture \n",
    "\n",
    "## Exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-harvest",
   "metadata": {},
   "source": [
    "#### Exercise 8.1\n",
    "\n",
    "1. <u>Spatial locality activity one</u>: \n",
    "Searching for appartments. Appartments within the same neighborhood usually have similar amendities, were built in the same time range, have the same age demographic and usually have a similar price range. Hence, when looking for appartment, it makes sense to tag listed appartement belonging to the same area into the same bookmark. <br><br>\n",
    "\n",
    "2. <u>Spatial locality activity two</u>:\n",
    "The search for a specific item at a grocery store involves spatial locatity. E.g. all pasta sauce are in the same alley. If someone if looking for a specific brand of pasta sauce, it is likely to be in that same alley. <br><br>\n",
    "\n",
    "3. <u>Temporal locality activity one and two</u>: \n",
    "Avoid traffic or finding parking can require temporal locality. On a given street, both the traffic and availability of parking follow a recurring pattern. For instance, if heavy traffic is experienced on a given road weekdays at 7:13am, the same heavy traffic will be expected 5 min later, or around the same time the next days. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-cinema",
   "metadata": {},
   "source": [
    "#### Exercise 8.2\n",
    "Most music streaming softwares (e.g. Youtube Music or Spotify) both uses spatial and temporal locality to make song recommendations or automatically build playlists: <br><br>\n",
    "<u>Spatial locality</u>: Recommended songs are usually 1. made by artists we already listen to (album proximity) 2. belong to the same genre of song listened to (genre proximity) 3. were listened often by users who have similar tastes (network proximity) or 4. belong to the list of top songs listened in the user's geographical location (physical proximity). <br>\n",
    "<u>Temporal locatility</u>: Recommended songs are often songs that were listened to by the user in the past. The more often a song is listened to, the more likely it is to be listened to again by the user. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-premium",
   "metadata": {},
   "source": [
    "#### Exercise 8.3 and 8.4\n",
    "\n",
    "\n",
    "\n",
    "When comparing a direct mapped cache vs. a fully associative cache when they both have blocks of 4 words and a tota size $C$ of 16 words: \n",
    "\n",
    "- The hit rate of the direct mapped cache can be made higher by repeatedly \n",
    "    1. Calling an address such that both cache miss and end up with different value, then ...\n",
    "    2. Calling the address that was previously overriden on the fully associative cache. <br>\n",
    "    An example of such sequence of address is: `0x0, 0x10, 0x20, 0x30, 0x50, 0x0, 0x70, 0x20, 0x60, 0x50` <br><br>\n",
    "\n",
    "- The hit rate of the associative cache can be made higher by calling the same two address that cause a conflict miss on the direct mapped cache. An example of such sequence of address is: `0x0, 0x10, 0x20, 0x30, 0x50, 0x10, 0x50, 0x10, 0x50, 0x10`\n",
    "\n",
    "\n",
    "<img src=\"./images/P8_3.PNG\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-fifty",
   "metadata": {},
   "source": [
    "#### Exercise 8.5\n",
    "\n",
    "(a) Increasing block size while keeping cache capacity and associativity constant result in a decrease in total number of block. This can result in an increase in miss rate if the total number of block is too small. Additionally, large block size take longer to retrieves, which further increases the mean CPI of loading instructions. Conversely, excessively small blocks also result in increases miss rate as they do not take advantage of spatial locality. \n",
    "\n",
    "\n",
    "\n",
    "(b) An increase in associativity results in a decrease in miss rate, primarily because higher associativity reduces likelihood of conflict miss. However higher associativity requires more hardware and thus is more expensive. Additionally, higher associativity results in higher delay, which might increase the average CPI of loading instructuctions. \n",
    "\n",
    "(c) The larger the cache size and the slower/expensive it gets. However, miss rate decreases too as cache size increases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-plain",
   "metadata": {},
   "source": [
    "#### Exercise 8.6\n",
    "\n",
    "The miss rate of a 2-way set associative cache is usually better than that of a direct mapped cache of the same block size and capacity, but not always. \n",
    "\n",
    "- It is usually better because 2-way set associative cache take advantage of both spatial and temporal locality whereas direct mapped only takes advantage of the former. \n",
    "- However, It cannot always better because when two caches have the same capacity but different configuration, they will store information differently, meaning there is always a sequence of address that can advantage one configuration over the other.\n",
    "\n",
    "Below is an example of address sequence pattern where a direct mapped cache where (C=4, N=1) has a lower miss rate than a two-way associative cache (C=4, N=2) <br><br> \n",
    "`0x0, 0x4, 0x8, 0xC` then repeat `0x10, 0x0, 0x8`\n",
    "\n",
    "\n",
    "<img src=\"images\\P8_6.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-success",
   "metadata": {},
   "source": [
    "#### Exercise 8.7\n",
    "\n",
    "(a) <br> \n",
    "False. The example provided in the previous exercise (8.6) proves it. <br> \n",
    "(b) <br> \n",
    "True. For a given block size, doubling the cache size of a direct mapped cache means halving the numbers of virtual address pointing to it. Hence, reducing the probability of a conflict miss by a factor 2. <br> \n",
    "(c) <br> \n",
    "True. Assuming a 32-bit memory system is used, an 8-byte block can only hold 2 consecutive instructions, whereas 32-byte blocks can hold 8 consecutive instructions. Since jump instructions are not common, most instruction are executed in the same order they are stored. Hence, larger block size should reduce the miss rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-georgia",
   "metadata": {},
   "source": [
    "#### Exercise 8.8\n",
    "\n",
    "(a) <br>\n",
    "capacity in terms of words is expressed as: $C = B \\times b = (N\\times S) b$ <br>\n",
    "\n",
    "(b) <br> \n",
    "tag size: A - $\\log_2S - \\text{Byte Offset}= A - \\log_2(B/N) - 2$\n",
    "\n",
    "(c) <br> \n",
    "For fully associative cache: <br>\n",
    "$S = 1$ <br>\n",
    "$N = B = C/b$\n",
    "\n",
    "(d) <br>\n",
    "For a direct mapped cache: <br>\n",
    "$S = B = C/b$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-electric",
   "metadata": {},
   "source": [
    "#### Exercise 8.9 \n",
    "The given <u>memory address</u> array <br> [0x40, 0x44, 0x48, 0x4C, 0x70, 0x74, 0x78, 0x7C, 0x80, 0x84, 0x88, 0x8C, 0x90, 0x94, 0x98, 0x9C, 0x0, 0x4, 0x8, 0xC, 0x10, 0x14, 0x18, 0x1C, 0x20] <br>\n",
    "can be converted to a more convenient <u>word address</u> array by dividing by 4 and changing hex to dec: <br>[16, 17, 18, 19, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39,  0, 1,  2,  3,  4,  5,  6,  7,  8]<br>\n",
    "\n",
    "The effective miss rate for (a), (b), (c) and (d) is 50%, 37.5%, 37.5% and 25% respectively.  \n",
    "\n",
    "The table below illustrate the mapping below word address and set address depending on the cache architecture. Yellow cell corresponds to miss. \n",
    "\n",
    "<img src=\"images\\P8_9.PNG\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-power",
   "metadata": {},
   "source": [
    "#### Exercise 8.10\n",
    "\n",
    "The given <u>memory address</u> array <br> [0x74, 0xA0, 0x78, 0x38C, 0xAC, 0x84, 0x88, 0x8C, 0x7C, 0x34, 0x38, 0x13C, 0x388, 0x18C] <br>\n",
    "can be converted to a more convenient <u>word address</u> array by dividing by 4 and changing hex to dec: <br>[116, 160, 120, 908, 172, 132, 136, 140, 124, 52, 56, 316, 904, 396]<br>\n",
    "\n",
    "The effective miss rate for (a), (b), (c) and (d) is 42.9%, 21.4%, 21.4% and 21.4% respectively.  \n",
    "\n",
    "The table below illustrate the mapping below word address and set address depending on the cache architecture. Yellow cell corresponds to miss. \n",
    "\n",
    "<img src=\"images\\P8_10.PNG\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-airline",
   "metadata": {},
   "source": [
    "#### Exercise 8.11\n",
    "\n",
    "[0x0, 0x8, 0x10, 0x18, 0x20, 0x28]\n",
    "\n",
    "(a) <br>\n",
    "C = 1028 bytes, b = 8 bytes, N = 1 <br>\n",
    "S = C / (bN) = 1028/8 = 128\n",
    "\n",
    "(b) <br>\n",
    "The miss rate should be 100%, which is only caused by compulsory miss. \n",
    "\n",
    "(c) <br>\n",
    "answer is (ii) <br>\n",
    "Increasing degree of associativity only reduces conflict miss, which do not happen in this pattern. <br>\n",
    "However, increasing block size allows to bring memory that is fetch on subsequent memory loads which does reduces miss rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-princess",
   "metadata": {},
   "source": [
    "#### Exercise 8.12\n",
    "(a) <br> \n",
    "The least two significant bits of any address (i.e. address$_{1:0}$) are ignored because a word is made up of 4 bytes. <br> For that same reason, a block size of 8 (b = 8, b'=3) would require 1 bit to specify the block position of a word, since two 4-bytes words can fit in 8 bytes. <br> \n",
    "Using the same logic, a block size of 16 (b = 16, b'=4) would require 2 bits to specify the block position. <br>\n",
    "Consequently, address bits corresponding to block position are address$_{[(b'-2)+2,2]}$ = address$_{[b',2]}$ \n",
    "\n",
    "\n",
    "(b) <br>\n",
    "The number of bits needed to identify sets corresponds to $\\log_2S = \\log_2\\big(\\dfrac{C}{bN}\\big) = \\log_2\\big(\\dfrac{2^c}{2^{b'}2^n}\\big) = c-b'-n$ <br> \n",
    "Hence, address bits corresponding to set position are address$_{[(c-b'-n)+b',b']}$ = address$_{[c-n,b']}$ \n",
    "\n",
    "(c) <br>\n",
    "Number of tag bits = Word length - Byte Offset - Block offset - Set address = 32 - 2 - (b'-2) - (c-b'-n) = 32 - c +n\n",
    "\n",
    "(d) <br>\n",
    "Each block has a tag. Hence, the total number of tag bits in the cache should be (32-c+n)C/b = (32-c+n)$2^{c-b'}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-video",
   "metadata": {},
   "source": [
    "#### Exercise 8.13\n",
    "(a) <br>\n",
    "Byte offset = $\\log_2(W/8)=\\log_2(4) = 2$ bit <br>\n",
    "Block offset = $\\log_2(b/W)=\\log_2(2) = 1$ bit <br> \n",
    "Set = $\\log_2(C/(Nb)) = \\log_2\\big(\\dfrac{2^{15} \\text{word}}{2\\times2\\text{word}}\\big) = 13$ bit <br>\n",
    "tag = A - Byte offset - Block offset - Set $= 32 - 2 - 1 - 13 = 16$ bit <br> \n",
    "\n",
    "(b) <br>\n",
    "Each block has an associated tag, hence: <br> \n",
    "16 bits $\\times$ B = 16 bits $\\times$ C/b = 16 bits $\\times \\dfrac{2^{15} \\text{word}}{2\\text{ word}} = 2^{18}$ bit or $2^{15}$ bytes \n",
    "\n",
    "(c) <br>\n",
    "set size = $N \\times (\\text{block size} + \\text{tag size} + \\text{D size}+ \\text{V size})$ <br>\n",
    "set size = $2 \\times (64 + 16 + 1 + 1) = 164$ bits <br>\n",
    "\n",
    "(d) <br>\n",
    "The provided SRAM blocks all contain $2^{14}$ entries/rows. They only allow for a single read input, hence at least N = 2 SRAM blocks are needed to allows for the 2-way associativity (2-way comparison). Consequently, the two `16kx20 SRAM` blocks used below are only used at full capacity. The LSB of their input refers to the block offset and should result in the same V,D and tag bits. Hence half the information `16kx20 SRAM` block contain is repeated. \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images\\P8_13.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-charter",
   "metadata": {},
   "source": [
    "#### Exercise 8.14\n",
    "(a) <br>\n",
    "The on-chip cache has a data capacity of $512\\times16 \\text{ bytes} = 512\\times4 \\text{ words} = 1024\\text{ words}$ <br>\n",
    "The Level 2 cache has a data capacity of $2^{18}\\times16 \\text{ bytes} = 2^{18}\\times4 \\text{ words} = 2^{20}\\text{ words}$<br>\n",
    "Assuming the processor accesses data at the word boundary, there are 1024 and $2^{20}$ different word location on the on-chip cache and level 2 cache respectively. \n",
    "\n",
    "\n",
    "(b) <br>\n",
    "$\\log_2(B) = \\log_2(512) = 9\\text{ bit}$ for the on-chip cache's block tag size: <br> \n",
    "$\\log_2(B) = \\log_2(2^{18}) = 18\\text{ bit}$ for the level 2 cache's block tag size: <br>\n",
    "\n",
    "(c) <br>\n",
    "\n",
    "$\\text{AMAT} = t_\\text{on-chip} + \\text{MR}_\\text{on-chip}(t_\\text{level 2} + \\text{MR}_\\text{level 2}t_\\text{main memory})$ <br>\n",
    "$\\text{AMAT} = t_\\text{a} + (1-A)(t_\\text{b} + (1-B)t_\\text{main memory})$\n",
    "\n",
    "(d) <br>\n",
    "This behavior is expected since the inital hit rate of the combined caches was already 98.5%: <br>\n",
    "1 - (1 - 85%)(1 - 90%) = 98.5% <br> \n",
    "In other words, enabling the on-chip cache result in 98.5% of the memory load being caught by one of the two caches (90% by the on-chip one and (1-90%)(85%) = 8.5% by the level 2 cache). Because the on-chip cache only holds a subset of the level 2 cache, disabling it should raise the level 2 cache hit rate to 90% + 8.5% = 98.5% \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-console",
   "metadata": {},
   "source": [
    "#### Exercise 8.15\n",
    "(a) A FIFO cache replacement policy should on average reduce the miss rate caused by conflict misses. <br>\n",
    "\n",
    "(b) The following access pattern on a fully associative cache of 4 blocks where each block contains 1 word and each word is 32 bits results in small miss rate for the FIFO replacement policy: <br>\n",
    "\n",
    "`0x4  0x8 0xC 0x10 0x4 0x14 0x8 0xC 0x10`\n",
    "\n",
    "<img src=\"images\\P8_15.PNG\" />\n",
    "\n",
    "Note that the initial 4 misses are compulsory, and thus inevitable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-diana",
   "metadata": {},
   "source": [
    "#### Exercise 8.16\n",
    "\n",
    "(a)<br>\n",
    "Because access time info is missing from the problem statement, the following assumption is made: <br>\n",
    "$t_\\text{data cache} = t_\\text{instr cache} = 0.25\\text{ns}$ (as used in table 7.6, p.389)\n",
    "\n",
    "$\\text{AMAT}_\\text{data cache} = t_\\text{data cache} + \\text{MR}_\\text{data cache}(t_\\text{main memory}) = 0.25 \\text{ns} +0.05(60\\text{ns}) = 3.25\\text{ns}$\n",
    "$\\text{AMAT}_\\text{instr cache} = t_\\text{instr cache} + \\text{MR}_\\text{instr cache}(t_\\text{main memory}) = 0.25 \\text{ns} $ <br>\n",
    "Consequently, the average memory access time should be anywhere between 0.25 and 3.25ns, base on the relative frequency of instruction read vs. data read. \n",
    "\n",
    "(b)<br>\n",
    "`lw` require 5 cycles for memory hits and 4 + (60ns / (1ns/cycle)) = 64 cycles for memory miss. On average, CPI should be 7.95. \n",
    "`sw` require 4 cycles for memory hits and 3 + (60ns / (1ns/cycle)) = 63 cycles for memory miss. On average, CPI should be 6.95\n",
    "\n",
    "(c) <br> \n",
    "CPI for load and store instruction were found to be 8 and 7 respectively in the previous question. \n",
    "Branch, jump and R-type instruction do not read or write on the data cache, hence their CPI are unaltered by the non-ideal memory system.  \n",
    "\n",
    "0.25(8) + 0.10(7) + 0.11(3) + 0.02(3) + 0.52(4) = 5.17 \n",
    "\n",
    "(d) <br> \n",
    "Same as (c), however every instruction gets their CPI incremented by 4.13 to account for their inital instruction load. \n",
    "\n",
    "1. Store instructions CPI: [0.07(60)+0.93(1)] + 3 + [0.05(60)+0.95(1)] = 12.08\n",
    "2. Load instructions CPI: [0.07(60)+0.93(1)] + 2 + [0.05(60)+0.95(1)] = 11.08\n",
    "3. Branch instructions CPI: [0.07(60)+0.93(1)] + 2 = 7.13\n",
    "4. Jump instructions CPI: [0.07(60)+0.93(1)]  + 2 =  7.13\n",
    "5. R-type instructions CPI: [0.07(60)+0.93(1)] + 3 = 8.13\n",
    " \n",
    "0.25(12.08) + 0.10(11.08) + 0.11(7.13) + 0.02(7.13) + 0.52(8.13) = 9.2825"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-rider",
   "metadata": {},
   "source": [
    "#### Exercise 8.17\n",
    "\n",
    "(a) <br> \n",
    "Because access time info is missing from the problem statement, the following assumption is made: <br>\n",
    "$t_\\text{data cache} = t_\\text{instr cache} = 0.25\\text{ns}$ (as used in table 7.6, p.389)\n",
    "\n",
    "$\\text{AMAT}_\\text{data cache} = t_\\text{data cache} + \\text{MR}_\\text{data cache}(t_\\text{main memory}) = 0.25 \\text{ns} +0.15(200\\text{ns}) = 30.25\\text{ns}$\n",
    "$\\text{AMAT}_\\text{instr cache} = t_\\text{instr cache} + \\text{MR}_\\text{instr cache}(t_\\text{main memory}) = 0.25 \\text{ns} $ <br>\n",
    "\n",
    "Consequently, the average memory access time should be anywhere between 0.25 and 30.25ns, base on the relative frequency of instruction read vs. data read. <br> \n",
    "\n",
    "(b)<br>\n",
    "`lw` require 5 cycles for memory hits and 4 + (200ns / (1ns/cycle)) = 204 cycles for memory miss. On the average the CPI should be $0.85(5\\text{ cycles}) + 0.15(204\\text{ cycles}) = 34.85$. <br>  \n",
    "`sw` require 4 cycles for memory hits and 3 + (200ns / (1ns/cycle)) = 203 cycles for memory miss.  On the average the CPI should be $0.85(5\\text{ cycles}) + 0.15(203\\text{ cycles}) = 34.7$. <br>\n",
    "\n",
    "(c) <br> \n",
    "CPI for load and store instruction were found to be 34.85 and 34.7 respectively in the previous question. \n",
    "Branch, jump and R-type instruction do not read or write on the data cache, hence their CPI are unaltered by the non-ideal memory system.  \n",
    "\n",
    "0.25(34.85) + 0.10(34.7) + 0.11(3) + 0.02(3) + 0.52(4) = 14.65 \n",
    "\n",
    "(d) <br> \n",
    "Same as (c), however every instruction gets their CPI incremented by 4.13 to account for their inital instruction load. \n",
    "\n",
    "1. Store instructions CPI: [0.07(200) +0.93(1)] + 3 + [0.05(200)+0.95(1)] = 28.88\n",
    "2. Load instructions CPI: [0.07(200)+0.93(1)] + 2 + [0.05(200)+0.95(1)] = 27.88\n",
    "3. Branch instructions CPI: [0.07(200)+0.93(1)] + 2 = 14.93\n",
    "4. Jump instructions CPI: [0.07(200)+0.93(1)]  + 2 =  14.93\n",
    "5. R-type instructions CPI: [0.07(200)+0.93(1)] + 3 = 15.93\n",
    "\n",
    "0.25(28.88) + 0.10(27.88) + 0.11(14.93) + 0.02(14.93) + 0.52(15.93) = 20.2325"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-reduction",
   "metadata": {},
   "source": [
    "#### Exercise 8.18\n",
    "Assuming each address points to a single byte, 64 bit addresssing can access up to $2^{64} \\times 1 \\text{ bytes} = 64\\times 2^{60} \\text{ bytes}$ or 16 exabyte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-configuration",
   "metadata": {},
   "source": [
    "#### Exercise 8.19\n",
    "The amount of physical and virtual memory of the computer should be: <br>\n",
    "$10^6\\$ \\big(\\dfrac{\\text{DRAM GB}}{10\\$}\\big) = 10^5 \\text{ DRAM GB}$  <br>\n",
    "$10^6\\$ \\big(\\dfrac{\\text{HDD GB}}{0.1\\$}\\big) = 10^7 \\text{ HDD GB}$\n",
    "\n",
    "The physical and virtual memory address size should exceed respectively: <br>\n",
    "$ 10^5 \\text{ DRAM GB} = 2^{30}\\cdot10^5 \\text{ DRAM byte}  = 2^{26}\\cdot10^5 \\text{ DRAM word} \\rightarrow\n",
    "\\log_2{(2^{26}\\cdot10^5)} = 42.609$ or $43$ bits <br> \n",
    "$ 10^7 \\text{ HDD GB} = 2^{30}\\cdot10^5 \\text{ DRAM byte}  = 2^{26}\\cdot10^7 \\text{ DRAM word} \\rightarrow\n",
    "\\log_2{(2^{26}\\cdot10^7)} = 49.25$ or $50$ bits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-footwear",
   "metadata": {},
   "source": [
    "#### Exercise 8.20\n",
    "\n",
    "(a) <br> \n",
    "$8\\text{MB} = 2^3 \\cdot 2^{20} (2^3\\text{ bits}) = 2^{26}\\text{ bits}$ <br>\n",
    "(b) <br> \n",
    "If the virtual memory can only address $2^{32}\\text{Byte}$, this means the virtual memory is effectively limited to $2^{32}$ Bytes in capacity: <br>\n",
    "$2^{32}\\text{Byte} / 4\\text{KB} = 2^{32}\\text{Byte} / 2^{12}\\text{Bytes} = 2^{20}\\text{ Pages}$ <br>\n",
    "(c) <br>\n",
    "$8\\text{MB} / 4\\text{KB} = 2^{23}\\text{Bytes} / 2^{12}\\text{Bytes} = 2^{11}\\text{ Pages}$ <br> \n",
    "(d) <br> \n",
    "20 bits and 11 bits respectively. <br> \n",
    "(e) <br> \n",
    "$2^{20}$ virtual pages mapping to $2^{11}$ physical pages means there are $2^9$ virtual pages per individual physical pages. Mapping virtual pages to physical pages using LSBs is a bad idead because adjacent memory addresses will always rely on different pages, meaning miss rate is going to increase. <br>\n",
    "(f) <br> \n",
    "As mentioned in section 8.4.2, a page table contains an entry for each virtual address. Therefore, the page table should have $2^{32}$  \n",
    "(d) <br> \n",
    "Each physical address has 23 bits, 12 of which are used as page offset, leaving 11 bits to define the physical page number. With the addition of a dirty bit D and valid bit D, each page table entry requires 13 bits. 2 bytes per entry (16 bits) should suffice. <br>\n",
    "(e) <br>\n",
    "$2^{20} \\text{Physical page} \\cdot 2 \\text{ Byte per virtual page entry} = 2\\cdot2^{20}\\text{ Byte} = 2\\text{ MB}$ <br>\n",
    "Below is an example of what the page table could look like. \n",
    "<img src=\"images\\P8_20.PNG\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-speech",
   "metadata": {},
   "source": [
    "#### Exercise 8.21\n",
    "(a) <br> \n",
    "$2 \\text{ GB} = 2 \\cdot 2^{30} \\cdot 8 \\text{ bits} = 2^{34} \\text{ bits}$ <br> \n",
    "(b) <br> \n",
    "$2^{50} \\text{ Byte} / 4\\text{ KB} = 2^{50} \\text{ Byte} / 2^{12}\\text{ Byte} = 2^{38}\\text{ Virutal pages}$ <br>\n",
    "(c) <br> \n",
    "$2\\text{ GB} / 4\\text{ KB} = 2^{31} \\text{ Byte} / 2^{12}\\text{ Byte} = 2^{19}\\text{ Physical pages}$ <br>\n",
    "(d) <br> \n",
    "$38$ and $19$ bits respectively. <br> \n",
    "(e) <br> \n",
    "There has to be a page table entry for each virtual page. Hence, there are $2^{38}$ entries. <br>\n",
    "(f) <br> \n",
    "Each physical page needs 19 bits to address. Combined with the D and V bits, 21 bits are needed per page table entry. 3 Byte should be sufficient. \n",
    "$2^{38} \\text{Physical page} \\cdot 3 \\text{ Byte per virtual page entry} = 768\\cdot2^{30}\\text{ Byte} = 768\\text{ GB}$ <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-claim",
   "metadata": {},
   "source": [
    "#### Exercise 8.22\n",
    "(a)<br> \n",
    "Before the TLB table: <br> \n",
    "\n",
    "Total running time = $ n \\cdot \\text{AMAT} $ <br>\n",
    "$= nt_\\text{cache} + n\\text{MR}_\\text{cache}t_\\text{main memory}\n",
    "+ n\\text{MR}_\\text{cache}(1-\\text{MR}_\\text{main memory})t_\\text{memory} +  n\\text{MR}_\\text{cache}\\text{MR}_\\text{main memory}t_\\text{HDD}$ <br> \n",
    "$= nt_\\text{cache} + n\\text{MR}_\\text{cache}\\big(t_\\text{main memory}\n",
    "+ (1-\\text{MR}_\\text{main memory})t_\\text{memory} +  \\text{MR}_\\text{main memory}t_\\text{HDD}\\big)$ <br>\n",
    "$\\implies \\text{AMAT} = t_\\text{cache} + \\text{MR}_\\text{cache}\\big(t_\\text{main memory}\n",
    "+ (1-\\text{MR}_\\text{main memory})t_\\text{memory} +  \\text{MR}_\\text{main memory}t_\\text{HDD}\\big)$ <br>\n",
    "$\\implies \\text{AMAT} = 1 + 0.02\\big(100 + (0.999997)100 +  0.000003(10^6)\\big) = 5.059994$ cycles <br> \n",
    "\n",
    "After adding the TLB table: <br> \n",
    "\n",
    "Total running time = $ n \\cdot \\text{AMAT} $ <br>\n",
    "$= nt_\\text{cache} + n\\text{MR}_\\text{cache}t_\\text{TLB}\n",
    "+ n\\text{MR}_\\text{cache}(1-\\text{MR}_\\text{TLB})t_\\text{main memory} +  n\\text{MR}_\\text{cache}\\text{MR}_\\text{TLB}t_\\text{main memory} + \n",
    "n\\text{MR}_\\text{cache}\\text{MR}_\\text{TLB}\\text{MR}_\\text{main memory}t_\\text{HDD}\n",
    "$ <br> \n",
    "$= nt_\\text{cache} + n\\text{MR}_\\text{cache}\\big(t_\\text{TLB}\n",
    "+ (1-\\text{MR}_\\text{TLB})t_\\text{main memory} +  \\text{MR}_\\text{TLB}t_\\text{main memory} + \n",
    "\\text{MR}_\\text{TLB}\\text{MR}_\\text{main memory}t_\\text{HDD}\\big)\n",
    "$ <br> \n",
    "$\\implies \\text{AMAT} = t_\\text{cache} + \\text{MR}_\\text{cache}\\big(t_\\text{TLB}\n",
    "+ (1-\\text{MR}_\\text{TLB})t_\\text{main memory} +  \\text{MR}_\\text{TLB}t_\\text{main memory} + \n",
    "\\text{MR}_\\text{TLB}\\text{MR}_\\text{main memory}t_\\text{HDD}\\big)\n",
    "$ <br> \n",
    "$\\implies \\text{AMAT} = 1 + 0.02\\big(1\n",
    "+ (0.9995)100 +  (0.0005)100 + \n",
    "(0.0005)(0.000003)(10^6)\\big)$ = 3.02003 cycles\n",
    " <br> \n",
    "\n",
    "\n",
    "(b) <br> \n",
    "- Given 32-bit addressing is used with 4KB pages, that means $\\log_2\\big(2^{32} \\text{bytes} / (2^{12} \\text{bytes/page})\\big) = $20 bits from the 32 bits are used to address the physical page and the remaining 12 bits are used as page offset. \n",
    "- Given there is 8MB of physical memory, this mean $\\log_2\\big(2^{23} \\text{bytes} / (2^{12} \\text{bytes/page})\\big) = $ 11 bits are needed to locate the physical pages and the remaining 12 bits are used as page offset. \n",
    "- Consequently each TLB entry requires $20 + 11 + 1 = 32$ bits (the added bit is used for the valid flag). <br>\n",
    "TLB size = $64 \\text{ entries} \\cdot 32 \\text{bit/entry} = 2048$ bits  \n",
    "\n",
    "(c) <br> \n",
    "<img src=\"images\\P8_22.PNG\" />\n",
    "\n",
    "(d) <br> \n",
    "In a fully associated cache, there is no varying input since all the address/data combinations saved have to be read simultaneously. Hence the SRAM has $2^0 = 1$ row. In terms of width, the only SRAM row needs to store all the $2048$ bit. Consequently, the SRAM size is $1\\times2048$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-stanford",
   "metadata": {},
   "source": [
    "#### Exercise 8.23\n",
    "\n",
    "(a) <br> \n",
    "- Physical address have a size of 34 bits. Given page are 4KB ($2^{12}$ bytes), 31 - 12 = 19 bits are needed to address physical pages. \n",
    "- Virtual address have a size of 50 bits. Given page are 4KB ($2^{12}$ bytes), 50 - 12 = 38 bits are needed to address virtual pages. \n",
    "- TLB size is thus $128 \\text{ entry} \\times (19 + 38 + 1) $ bits = 7424 bits.  \n",
    "\n",
    "(b) <br> \n",
    "Illustration identical to 8.22 (c) with the following adjustements: <br>\n",
    "- Virtual page number size: 20 bits replaced with 38 bits \n",
    "- Physical page number size: 11 bits replaced with 19 bits\n",
    "- Multiplexer: 64 to 1 replaced with 128 to 1. \n",
    "\n",
    "\n",
    "(c) <br> \n",
    "The SRAM size is $1\\times7424$ (same logic used as in 8.22 part (d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-mouse",
   "metadata": {},
   "source": [
    "#### Exercise 8.24\n",
    "\n",
    "(a) <br> \n",
    "The TLB can be illustrated as part of the memory system, which replaces the `data/instruction` block of the original multicycle processor presented in chapter 7. <br>\n",
    "As shown below, TLB is not needed provided no cache miss occurs. However, if a cache miss does occur, the TLB is used to map the virtual address to the physical address. If the TLB does not miss, only one cycle is added to the read operation. If the TLB misses, two cycles are added to the read operating: one to access the page table, the othe to access the physical address.  The illustration below assumes there Main memory miss rate of 0%. \n",
    "<img src=\"images\\P8_24.PNG\" />\n",
    "\n",
    "(b) <br> \n",
    "Assuming TLB is sized such that one TLB read take 1 cycle, every instruction requiring a memory read or write will have their CPI increase by at least 1 to account for the added TLB read operation. If the TLB misses, the CPI of the instruction increases by 2. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-visitor",
   "metadata": {},
   "source": [
    "#### Exercise 8.25\n",
    "(a) <br> \n",
    "- 25 bits virtual addresses with 64KB pages ($2^{16}$ bytes) means virtual page address require $25 - 16 = 9$ bits. <br>\n",
    "- 22 bits physical addresses with 64KB pages ($2^{16}$ bytes) means virtual page address require $22 - 16 = 6$ bits. <br>\n",
    "- Each page table row requires space for the virtual page number (9 bit), physical page number (6 bit), valid bit (1 bit) and dirty bit (1 bit), giving a total of 9 + 6 + 1 + 1 = 17 bits. \n",
    "- The table page has a row for each virtual page. Hence the total size of the page table is $17 \\cdot 2^9 = 8704$ bits <br> \n",
    "\n",
    "(b) <br> \n",
    "- Reducing the page size from 16KB to 4KB will increase the page table size by a factor of 4.94 (4 in height and 21/17=1.235 in width). Since the page table is located in dedicated hardware, this means almost 5 times more SRAM memory will be needed to map the same amount of physical memory. \n",
    "- Additionally, in order for the TLB to maintain the same hit rate, it needs to cover the same fraction of the used virtual memory, meaning it has to map 4 times more table. Recall that TLB rely on fully associative memory, therefore 4 times more pages have to be compared. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-alfred",
   "metadata": {},
   "source": [
    "#### Exercise 8.26\n",
    "\n",
    "- In general, virtual memory should have a limited impact on the execution of a program, since it is assumed the program will be loaded into its physical memory as it is being executed. Although, one way to optimize speed is to make sure a given program spans across the least amount of pages to minimize the number of page loads. <br><br>\n",
    "\n",
    "- In terms of physical memory, size has little impact on the speed of a program that is ran, provided it is large enough to store every program that are being ran concurrently, such that the physical memory miss rate is close to 0%. <br><br>\n",
    "\n",
    "- Page size need to be optimized. If the page size is too small, more page loads will be needed per program which will slow down execution. On the other hand, if the page size is too big, page loads will introduce a greater amount of data/instruction into the physical memory that is not needed, which increase the number of page load to compensate, and thus also slows down execution. <br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-bargain",
   "metadata": {},
   "source": [
    "#### Exercise 8.27\n",
    "(a) <br> \n",
    "The maximum amount of virtual memory accessible by a single program depends on 1. how to operating systems allocate virtual memory between the programs that are running but also 2. how much virtual memory is available on the HDD or SSD. At most, a program could access all the available memory, (i.e. $2^{32}$ byte = 4GB) provided the HDD or SSD's capacity exceeds 4GB and it is the only program running.  \n",
    "\n",
    "(b) <br> \n",
    "The size of the hard drive does not significanly affect performance since the vast majority of memory read operations happen at the cache level, few of them happen at the physical memory (RAM) level and extremely few of them happen at the virtual memory level. \n",
    "\n",
    "(c) <br> \n",
    "Physical memory size affects performance by a larger amount than virtual memory size. However, the impact of physical memory size is especially noticeable when multiple programs are ran simultaneously or when large amounts of data are processed simultenously). Hence, provided the physical memory size is large enough for the application used, increments in its size will at most negligibly increase performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-research",
   "metadata": {},
   "source": [
    "#### Exercise 8.28\n",
    "(a) <br> \n",
    "MIPS instruction to read the button and activate light pattern accordingly: <br> \n",
    "```\n",
    "\n",
    "    addi   $t2 0x16($0)           # initialize value of LED pattern when turned on(10110 = 0x16)\n",
    "start:\n",
    "    lw     $t1 0xFFFFFF10($0)     # store button status in register $t1\n",
    "    beq    $t1 $0 unpressed            \n",
    "    sw     $t2 0xFFFFFF14($0)     # button pressed: write pattern 4'b10110 \n",
    "    j      start                  # jump back to head of loop\n",
    "unpressed: \n",
    "    sw     $0  0xFFFFFF14($0)     # button unpressed: write 4'b00000 \n",
    "    j      start                  # jump back to head of loop\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "(b) <br> \n",
    "<img src=\"images\\P8_28.PNG\" />\n",
    "(c) \n",
    "<b> Verilog Code</b> \n",
    "```\n",
    "module AddressDecoder(input logic clk,\n",
    "                  input logic memwrite, \n",
    "                  input logic [31:0] address, \n",
    "                  output logic WEM,           // WEM signal to write o memory block \n",
    "                  output logic [2]RDsel,      // multiplexor input for \n",
    "                  output logic WE1            // WE1 signal to write on LED register\n",
    "                  ); \n",
    "logic comp;\n",
    "assign comp = (address == 32'hFFFFFF10);\n",
    "\n",
    "always @(posedge clk) \n",
    "  begin\n",
    "    if (address == 32'hFFFFFF10) begin // read button position\n",
    "        {RDsel, WEM, WE1} <= {2'b01, 1'b0, 1'b0};\n",
    "      end\n",
    "    if (address == 32'hFFFFFF14) begin // LEDs write   \n",
    "        {RDsel, WEM, WE1} <= {2'b10, 1'b0, memwrite};\n",
    "      end \n",
    "    else                         \n",
    "      begin // Read memory block \n",
    "        {RDsel, WEM, WE1} <= {2'b00, memwrite, 1'b0};\n",
    "      end\n",
    "  end\n",
    "endmodule \n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-chosen",
   "metadata": {},
   "source": [
    "#### Exercise 8.29\n",
    "\n",
    "\n",
    "\n",
    "(a) <br>\n",
    "```\n",
    "    addi   $t1 0($0)             # Initialize position of state  \n",
    "begin: \n",
    "    j      waitforXseconds       # jump to whichever subroutine and come back \n",
    "    bne    $t1 $0 S1             # check if state 0 \n",
    "    addi   $t3 0x21($0)          # compute output {Lb,La} = {100,001} = 0x21\n",
    "    sw     $t3 0xFFFFF04($0)     # apply output \n",
    "    lw     $t0 0xFFFFF000($0)    # retrieve value of {Ta,Tb}\n",
    "    andi   $t0 0x2($t0)          # mask Ta with 2'b10\n",
    "    bne    $t0 $0 begin          # if Ta != 0 remain in state 0 \n",
    "    addi   $t1 0x1($t1)          # increment state\n",
    "    j      begin\n",
    "S1:\n",
    "    addi   $t5 0x1($0)           # set comparison state as 1\n",
    "    bne    $t1 $t5 S2            # check if state = S1\n",
    "    addi   $t3 0x22($0)          # compute output {Lb,La} = {100,010} = 0x22\n",
    "    sw     $t3 0xFFFFF04($0)     # apply output \n",
    "    addi   $t1 0x1($t1)           # increment state \n",
    "    j      begin\n",
    "S2:\n",
    "    addi   $t5 0x2($0)           # set comparison state as 2\n",
    "    bne    $t1 $0 S3             # check if state = S2\n",
    "    addi   $t3 0xC($0)           # compute output {Lb,La} = {001,100} = 0xC\n",
    "    sw     $t3 0xFFFFF04($0)     # apply output \n",
    "    lw     $t0 0xFFFFF000($0)    # retrieve value of {Ta,Tb}\n",
    "    andi   $t0 0x1($t0)          # mask Tb with 2'b01\n",
    "    bne    $t0 $0 begin          # if Tb != 0 remain in state 0 \n",
    "    addi   $t1 0x1($t1)          # increment state\n",
    "    j      begin\n",
    "S3: \n",
    "    addi   $t5 0x1($0)           # set comparison state as 3\n",
    "    addi   $t3 0x14($0)          # compute output {Lb,La} = {010,100} = 0x14\n",
    "    sw     $t3 0xFFFFF04($0)     # apply output \n",
    "    addi   $t1 0x0($0)           # increment state back to S0\n",
    "    j      begin\n",
    "\n",
    "```\n",
    "\n",
    "note: There has to be an added subroutine that gets called everytime the loop begins to allow for a defined time interval between state transition. \n",
    "\n",
    "\n",
    "(b) <br>\n",
    "<img src=\"images\\P8_29.PNG\" />\n",
    "\n",
    "(c) <br> \n",
    "\n",
    "```\n",
    "module AddressDecoder2(input logic clk,\n",
    "                  input logic memwrite, \n",
    "                  input logic [31:0] address, \n",
    "                  output logic WEM,           // WEM signal to write o memory block \n",
    "                  output logic [2]RDsel,      // multiplexor input for \n",
    "                  output logic WE1            // WE1 signal to write on street light register\n",
    "                  ); \n",
    "logic comp;\n",
    "assign comp = (address == 32'hFFFFFF10);\n",
    "\n",
    "always @(posedge clk) \n",
    "  begin\n",
    "    if (address == 32'hFFFFF000) begin // Ta/Tb inputs read \n",
    "        {RDsel, WEM, WE1} <= {2'b01, 1'b0, 1'b0};\n",
    "      end\n",
    "    if (address == 32'hFFFFF004) begin // Street light right   \n",
    "        {RDsel, WEM, WE1} <= {2'b10, 1'b0, memwrite};\n",
    "      end \n",
    "    else                         \n",
    "      begin // Read memory block \n",
    "        {RDsel, WEM, WE1} <= {2'b00, memwrite, 1'b0};\n",
    "      end\n",
    "  end\n",
    "endmodule \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-percentage",
   "metadata": {},
   "source": [
    "#### Exercise 8.30\n",
    "\n",
    "\n",
    "(a) <br>\n",
    "\n",
    "```\n",
    "    addi $t0 0x0($0)           # initialize state to S0\n",
    "begin: \n",
    "    sw   $t1 0xFFFFFF040($0)   # retrieve {Y,A} register \n",
    "    andi $t2 0x1($t1)          # mask {Y,A} with 2'b01 to isolate input A\n",
    "    bne  $t0 $0 S1             # check if State == S0\n",
    "    addi $t1 0xFFFFFFFD($t1)   # force Y to 0 without touching other bits\n",
    "    sw   $t1 0xFFFFFF040($0)   # force Y to 0 without touching other bits\n",
    "    bne  $t2 $0 begin          # if A != 0, stay in S0\n",
    "    addi $t0 0x1($t0)          # if A == 0, set state++\n",
    "    j begin\n",
    "S1: \n",
    "    addi $t3 0x1($0)           # i = 1 \n",
    "    bne  $t0 $t3 S2            # check if State == S0\n",
    "    addi $t1 0xFFFFFFFD($t1)   # force Y to 0 without touching other bits\n",
    "    sw   $t1 0xFFFFFF040($0)   # force Y to 0 without touching other bits  \n",
    "    beq  $t2 $0 begin          # if A == 0, stay in S1\n",
    "    addi $t0 0x1($t0)          # if A == 0, set state++ \n",
    "    j begin\n",
    "S2: \n",
    "    ori  $t1 0x0000002($t1)    # force Y to 1 without touching other bits\n",
    "    sw   $t1 0xFFFFFF040($0)   # force Y to 1 without touching other bits \n",
    "    beq  $t2 $0 case           # if A == 0, go back to S1\n",
    "    addi $t0 0x0($0)           # if A == 1, go to S0\n",
    "    j begin\n",
    "case: \n",
    "    addi $t0 0x1($0)           # if A == 0, go back to S1\n",
    "    j begin\n",
    "    \n",
    " ```\n",
    "(b) <br> \n",
    "<img src=\"images\\P8_30.PNG\" />\n",
    "\n",
    "(c) <br> \n",
    "\n",
    "\n",
    "```\n",
    "module AddressDecoder3(input logic clk,\n",
    "                  input logic memwrite, \n",
    "                  input logic [31:0] address, \n",
    "                  output logic WEM,           // WEM signal to write o memory block \n",
    "                  output logic [1]RDsel,      // multiplexor input for \n",
    "                  output logic WE1            // WE1 signal to write on {Y,A} register\n",
    "                  ); \n",
    "logic comp;\n",
    "assign comp = (address == 32'hFFFFFF10);\n",
    "\n",
    "always @(posedge clk) \n",
    "  begin\n",
    "    if (address == 32'hFFFFF040) begin // {Y,A} inputs read \n",
    "        {RDsel, WEM, WE1} <= {1'b1, 1'b0, memwrite};\n",
    "      end\n",
    "    else                         \n",
    "      begin // Read memory block \n",
    "        {RDsel, WEM, WE1} <= {1'b0, memwrite, 1'b0};\n",
    "      end\n",
    "  end\n",
    "endmodule \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
