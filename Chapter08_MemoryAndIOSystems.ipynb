{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afraid-scope",
   "metadata": {},
   "source": [
    "## Chapter 8 - Microarchitecture \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-harvest",
   "metadata": {},
   "source": [
    "#### Exercise 8.1\n",
    "\n",
    "1. <u>Spatial locality activity one</u>: \n",
    "Searching for appartments. Appartments within the same neighborhood usually have similar amendities, where built in the same time range, have the same age demographic and usually have a similar price range. Hence, when looking for appartment, it makes sense to tag listed appartement belonging to the same area into the same bookmark. <br><br>\n",
    "\n",
    "2. <u>Spatial locality activity two</u>:\n",
    "The search for a specific item at a grocery store involves spatial locatity. \n",
    "\n",
    "3. Temporal locality activity one: \n",
    "\n",
    "4. Temporal locality activity two: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-cinema",
   "metadata": {},
   "source": [
    "#### Exercise 8.2\n",
    "Most music streaming softwares (e.g. Youtube Music or Spotify) both uses spatial and temporal locality to make song recommendations or automatically build playlists: <br><br>\n",
    "<u>Spatial locality</u>: Recommended songs are usually 1. made by artists we already listen to (album proximity) 2. belong to the same genre of song listened to (genre proximity) 3. were listened often by users who have similar tastes (network proximity) or 4. belong to the list of top songs listened in the user's geographical location (physical proximity). <br>\n",
    "<u>Temporal locatility</u>: Recommended songs are often songs that were listened to by the user in the past. The more often a song is listened to, the more likely it is to be listened to again by the user. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-premium",
   "metadata": {},
   "source": [
    "#### Exercise 8.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-working",
   "metadata": {},
   "source": [
    "#### Exercise 8.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-fifty",
   "metadata": {},
   "source": [
    "#### Exercise 8.5\n",
    "\n",
    "(a) Increasing block size while keeping cache capacity and associativity constant results in a decrease in the total number of sets. Conflict miss is more likely to occur if the block size is too large whereas\n",
    "\n",
    "(b) An increase in associativity results in a decrease in miss rate - primarily because higher associativity reduces likelihood of conflict miss. However higher associativity requires more hardware and is thus more expensive. \n",
    "\n",
    "\n",
    "(c) The larger the cache size, and the slower/expensive it gets. However, miss rate decreases too as cache size increases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-plain",
   "metadata": {},
   "source": [
    "#### Exercise 8.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-success",
   "metadata": {},
   "source": [
    "#### Exercise 8.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-georgia",
   "metadata": {},
   "source": [
    "#### Exercise 8.8\n",
    "\n",
    "(a) <br>\n",
    "capacity in terms of words is expressed as: $C = B \\times b = (N\\times S) b$ <br>\n",
    "capacity in terms of bit is exprssed as: $C \\times A$\n",
    "\n",
    "(b) <br> \n",
    "tag size: $\\log_2S = \\log_2(B/N)$\n",
    "\n",
    "(c) <br> \n",
    "For fully associative cache: <br>\n",
    "$S = 1$ <br>\n",
    "$N = B = C/b$\n",
    "\n",
    "(d) <br>\n",
    "For a direct mapped cache: <br>\n",
    "$S = B = C/b$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-electric",
   "metadata": {},
   "source": [
    "#### Exercise 8.9 \n",
    "The given <u>memory address</u> array [0x40, 0x44, 0x48, 0x4C, 0x70, 0x74, 0x78, 0x7C, 0x80, 0x84, 0x88, 0x8C, 0x90, 0x94, 0x98, 0x9C, 0x0, 0x4, 0x8, 0xC, 0x10, 0x14, 0x18, 0x1C, 0x20] can be converted to a more convenient <u>word address</u> array by dividing by 4 and changing converting hex to dec:[16, 17, 18, 19, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39,  0, 1,  2,  3,  4,  5,  6,  7,  8]\n",
    "\n",
    "The effective miss rate for (a), (b), (c) and (d) is 50%, 37.5%, 37.5% and 25% respectively.  \n",
    "\n",
    "The table below illustrate the mapping below word address and set address depending on the cache architecture. Yellow cell corresponds to miss. \n",
    "\n",
    "<img src=\"images\\P8_9.PNG\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-power",
   "metadata": {},
   "source": [
    "#### Exercise 8.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-airline",
   "metadata": {},
   "source": [
    "#### Exercise 8.11\n",
    "\n",
    "[0x0, 0x8, 0x10, 0x18, 0x20, 0x28]\n",
    "\n",
    "(a) <br>\n",
    "C = 1028 bytes, b = 8 bytes, N = 1 <br>\n",
    "S = C / (bN) = 1028/8 = 128\n",
    "\n",
    "(b) <br>\n",
    "The miss rate should be 100%, which is only caused by compulsory miss. \n",
    "\n",
    "(c) <br>\n",
    "answer is (ii) <br>\n",
    "Increasing degree of associativity only reduces conflict miss, which do not happen in this pattern. <br>\n",
    "However, increasing block size allows to bring memory that is fetch on subsequent memory loads which does reduces miss rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-princess",
   "metadata": {},
   "source": [
    "#### Exercise 8.12\n",
    "(a) <br> \n",
    "The least two significant bits of any address (i.e. address$_{1:0}$) are ignored because a word is made up of 4 bytes. <br> For that same reason, a block size of 8 (b = 8, b'=3) would require 1 bit to specify the block position of a word, since two 4-bytes words can fit in 8 bytes. <br> \n",
    "Using the same logic, a block size of 16 (b = 16, b'=4) would require 2 bits to specify the block position. <br>\n",
    "Consequently, address bits corresponding to block position are address$_{[(b'-2)+2,2]}$ = address$_{[b',2]}$ \n",
    "\n",
    "\n",
    "(b) <br>\n",
    "The number of bits needed to identify sets corresponds to $\\log_2S = \\log_2\\big(\\dfrac{C}{bN}\\big) = \\log_2\\big(\\dfrac{2^c}{2^{b'}2^n}\\big) = c-b'-n$ <br> \n",
    "Hence, address bits corresponding to set position are address$_{[(c-b'-n)+b',b']}$ = address$_{[c-n,b']}$ \n",
    "\n",
    "(c) <br>\n",
    "Number of tag bits = Word length - Byte Offset - Block offset - Set address = 32 - 2 - (b'-2) - (c-b'-n) = 32 - c +n\n",
    "\n",
    "(d) <br>\n",
    "Each block has a tag. Hence, the total number of tag bits in the cache should be (32-c+n)C/b = (32-c+n)$2^{c-b'}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-video",
   "metadata": {},
   "source": [
    "#### Exercise 8.13\n",
    "(a) <br>\n",
    "Byte offset = $\\log_2(W/8)=\\log_2(4) = 2$ bit <br>\n",
    "Block offset = $\\log_2(b/W)=\\log_2(2) = 1$ bit <br> \n",
    "Set = $\\log_2(C/(Nb)) = \\log_2\\big(\\dfrac{2^{15} \\text{word}}{2\\times2\\text{word}}\\big) = 13$ bit <br>\n",
    "tag = A - Byte offset - Block offset - Set $= 32 - 2 - 1 - 13 = 16$ bit <br> \n",
    "\n",
    "(b) <br>\n",
    "Each block has an associated tag, hence: <br> \n",
    "16 bits $\\times$ B = 16 bits $\\times$ C/b = 16 bits $\\times \\dfrac{2^{15} \\text{word}}{2\\text{ word}} = 2^{18}$ bit or $2^{15}$ bytes \n",
    "\n",
    "(c) <br>\n",
    "set size = $N \\times (\\text{block size} + \\text{tag size} + \\text{D size}+ \\text{V size})$ <br>\n",
    "set size = $2 \\times (64 + 16 + 1 + 1) = 164$ bits <br>\n",
    "\n",
    "(d) <br>\n",
    "The provided SRAM blocks all contain $2^{14}$ entries/rows. They only allow for a single read input, hence at least N = 2 SRAM blocks are needed to allows for the 2-way associativity (2-way comparison). Consequently, the two `16kx20 SRAM` blocks used below are only used at full capacity. The LSB of their input refers to the block offset and should result in the same V,D and tag bits. Hence half the information `16kx20 SRAM` block contain is repeated. \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images\\P8_13.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-charter",
   "metadata": {},
   "source": [
    "#### Exercise 8.14\n",
    "(a) <br>\n",
    "The on-chip cache has a data capacity of $512\\times16 \\text{ bytes} = 512\\times4 \\text{ words} = 1024\\text{ words}$ <br>\n",
    "The Level 2 cache has a data capacity of $2^{18}\\times16 \\text{ bytes} = 2^{18}\\times4 \\text{ words} = 2^{20}\\text{ words}$<br>\n",
    "Assuming the processor accesses data at the word boundary, there are 1024 and $2^{20}$ different word location on the on-chip cache and level 2 cache respectively. \n",
    "\n",
    "\n",
    "(b) <br>\n",
    "$\\log_2(B) = \\log_2(512) = 9\\text{ bit}$ for the on-chip cache's block tag size: <br> \n",
    "$\\log_2(B) = \\log_2(2^{18}) = 18\\text{ bit}$ for the level 2 cache's block tag size: <br>\n",
    "\n",
    "(c) <br>\n",
    "\n",
    "$\\text{AMAT} = t_\\text{on-chip} + \\text{MR}_\\text{on-chip}(t_\\text{level 2} + \\text{MR}_\\text{level 2}t_\\text{main memory})$ <br>\n",
    "$\\text{AMAT} = t_\\text{a} + (1-A)(t_\\text{b} + (1-B)t_\\text{main memory})$\n",
    "\n",
    "(d) <br>\n",
    "This behavior is expected since the inital hit rate of the combined caches was already 98.5%: <br>\n",
    "1 - (1 - 85%)(1 - 90%) = 98.5% <br> \n",
    "In other words, enabling the on-chip cache result in 98.5% of the memory load being caught by one of the two caches (90% by the on-chip one and (1-90%)(85%) = 8.5% by the level 2 cache). Because the on-chip cache only holds a subset of the level 2 cache, disabling it should raise the level 2 cache hit rate to 90% + 8.5% = 98.5% \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-console",
   "metadata": {},
   "source": [
    "#### Exercise 8.15\n",
    "(a) A FIFO cache replacement policy should on average reduce the miss rate caused by conflict misses. <br>\n",
    "\n",
    "(b) The following access pattern on a fully associative cache of 4 blocks where each block contains 1 word and each word is 32 bits results in small miss rate for the FIFO replacement policy: <br>\n",
    "\n",
    "`0x4  0x8 0xC 0x10 0x4 0x14 0x8 0xC 0x10`\n",
    "\n",
    "<img src=\"images\\P8_15.PNG\" />\n",
    "\n",
    "Note that the initial 4 misses are compulsory, and thus inevitable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-diana",
   "metadata": {},
   "source": [
    "#### Exercise 8.16\n",
    "\n",
    "(a)<br>\n",
    "Because access time info is missing from the problem statement, the following assumption is made: <br>\n",
    "$t_\\text{data cache} = t_\\text{instr cache} = 0.25\\text{ns}$ (as used in table 7.6, p.389)\n",
    "\n",
    "$\\text{AMAT}_\\text{data cache} = t_\\text{data cache} + \\text{MR}_\\text{data cache}(t_\\text{main memory}) = 0.25 \\text{ns} +0.05(60\\text{ns}) = 3.25\\text{ns}$\n",
    "$\\text{AMAT}_\\text{instr cache} = t_\\text{instr cache} + \\text{MR}_\\text{instr cache}(t_\\text{main memory}) = 0.25 \\text{ns} $ <br>\n",
    "Consequently, the average memory access time should be anywhere between 0.25 and 3.25ns, base on the relative frequency of instruction read vs. data read. \n",
    "\n",
    "(b)<br>\n",
    "`lw` require 5 cycles for memory hits and 4 + (60ns / (1ns/cycle)) = 64 cycles for memory miss. On the average the CPI should be 7.95. \n",
    "`sw` require 4 cycles for memory hits and 3 + (60ns / (1ns/cycle)) = 63 cycles for memory miss. On the average the CPI should be 6.95\n",
    "\n",
    "(c) <br> \n",
    "CPI for load and store instruction were found to be 8 and 7 respectively in the previous question. \n",
    "Branch, jump and R-type instruction do not read or write on the data cache, hence their CPI are unaltered by the non-ideal memory system.  \n",
    "\n",
    "0.25(8) + 0.10(7) + 0.11(3) + 0.02(3) + 0.52(4) = 5.17 \n",
    "\n",
    "(d) <br> \n",
    "Same as (c), however every instruction gets their CPI incremented by 4.13 to account for their inital instruction load. \n",
    "\n",
    "1. Store instructions CPI: [0.07(60)+0.93(1)] + 3 + [0.05(60)+0.95(1)] = 12.08\n",
    "2. Load instructions CPI: [0.07(60)+0.93(1)] + 2 + [0.05(60)+0.95(1)] = 11.08\n",
    "3. Branch instructions CPI: [0.07(60)+0.93(1)] + 2 = 7.13\n",
    "4. Jump instructions CPI: [0.07(60)+0.93(1)]  + 2 =  7.13\n",
    "5. R-type instructions CPI: [0.07(60)+0.93(1)] + 3 = 8.13\n",
    "\n",
    "0.25(12.08) + 0.10(11.08) + 0.11(7.13) + 0.02(7.13) + 0.52(8.13) = 9.2825"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-rider",
   "metadata": {},
   "source": [
    "#### Exercise 8.17\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-reduction",
   "metadata": {},
   "source": [
    "#### Exercise 8.18\n",
    "Assuming each address points to a single byte, 64 bit addresssing can access up to $2^{64} \\times 1 \\text{ bytes} = 64\\times 2^{60} \\text{ bytes}$ or 16 exabyte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-configuration",
   "metadata": {},
   "source": [
    "#### Exercise 8.19\n",
    "The amount of physical and virtual memory of the computer should be: <br>\n",
    "$10^6\\$ \\big(\\dfrac{\\text{DRAM GB}}{10\\$}\\big) = 10^5 \\text{ DRAM GB}$  <br>\n",
    "$10^6\\$ \\big(\\dfrac{\\text{HDD GB}}{0.1\\$}\\big) = 10^7 \\text{ HDD GB}$\n",
    "\n",
    "The physical and virtual memory address size should exceed respectively: <br>\n",
    "$ 10^5 \\text{ DRAM GB} = 2^{30}\\cdot10^5 \\text{ DRAM byte}  = 2^{26}\\cdot10^5 \\text{ DRAM word} \\rightarrow\n",
    "\\log_2{(2^{26}\\cdot10^5)} = 42.609$ or $43$ bits <br> \n",
    "$ 10^7 \\text{ HDD GB} = 2^{30}\\cdot10^5 \\text{ DRAM byte}  = 2^{26}\\cdot10^7 \\text{ DRAM word} \\rightarrow\n",
    "\\log_2{(2^{26}\\cdot10^7)} = 49.25$ or $50$ bits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-footwear",
   "metadata": {},
   "source": [
    "#### Exercise 8.20\n",
    "\n",
    "(a) <br> \n",
    "$8\\text{MB} = 2^3 \\cdot 2^{20} (2^3\\text{ bits}) = 2^{26}\\text{ bits}$ <br>\n",
    "(b) <br> \n",
    "If the virtual memory can only address $2^{32}\\text{Byte}$, this means the virtual memory is effectively limited to $2^{32}$ Bytes in capacity: <br>\n",
    "$2^{32}\\text{Byte} / 4\\text{KB} = 2^{32}\\text{Byte} / 2^{12}\\text{Bytes} = 2^{20}\\text{ Pages}$ <br>\n",
    "(c) <br>\n",
    "$8\\text{MB} / 4\\text{KB} = 2^{23}\\text{Bytes} / 2^{12}\\text{Bytes} = 2^{11}\\text{ Pages}$ <br> \n",
    "(d) <br> \n",
    "20 bits and 11 bits respectively. <br> \n",
    "(e) <br> \n",
    "$2^{20}$ virtual pages mapping to $2^{11}$ physical pages means there are $2^9$ virtual pages per individual physical pages. Mapping virtual pages to physical pages using LSBs is a bad idead because adjacent memory addresses will always rely on different pages, meaning miss rate is going to increase. <br>\n",
    "(f) <br> \n",
    "As mentioned in section 8.4.2, a page table contains an entry for each virtual address. Therefore, the page table should have $2^{32}$  \n",
    "(d) <br> \n",
    "Each physical address has 23 bits, 12 of which are used as page offset, leaving 11 bits to define the physical page number. With the addition of a dirty bit D and valid bit D, each page table entry requires 13 bits. 2 bytes per entry (16 bits) should suffice. <br>\n",
    "(e) <br>\n",
    "$2^{20} \\text{Physical page} \\cdot 2 \\text{ Byte per virtual page entry} = 2\\cdot2^{20}\\text{ Byte} = 2\\text{ MB}$ <br>\n",
    "Below is an example of what the page table could look like. \n",
    "<img src=\"images\\P8_20.PNG\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-speech",
   "metadata": {},
   "source": [
    "#### Exercise 8.21\n",
    "(a) <br> \n",
    "$2 \\text{ GB} = 2 \\cdot 2^{30} \\cdot 8 \\text{ bits} = 2^{34} \\text{ bits}$ <br> \n",
    "(b) <br> \n",
    "$2^{50} \\text{ Byte} / 4\\text{ KB} = 2^{50} \\text{ Byte} / 2^{12}\\text{ Byte} = 2^{38}\\text{ Virutal pages}$ <br>\n",
    "(c) <br> \n",
    "$2\\text{ GB} / 4\\text{ KB} = 2^{31} \\text{ Byte} / 2^{12}\\text{ Byte} = 2^{19}\\text{ Physical pages}$ <br>\n",
    "(d) <br> \n",
    "$38$ and $19$ bits respectively. <br> \n",
    "(e) <br> \n",
    "There has to be a page table entry for each virtual page. Hence, there are $2^{38}$ entries. <br>\n",
    "(f) <br> \n",
    "Each physical page needs 19 bits to address. Combined with the D and V bits, 21 bits are needed per page table entry. 3 Byte should be sufficient. \n",
    "$2^{38} \\text{Physical page} \\cdot 3 \\text{ Byte per virtual page entry} = 768\\cdot2^{30}\\text{ Byte} = 768\\text{ GB}$ <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-claim",
   "metadata": {},
   "source": [
    "#### Exercise 8.22\n",
    "(a)<br> \n",
    "Before the TLB table: <br> \n",
    "\n",
    "Total running time = $ n \\cdot \\text{AMAT} $ <br>\n",
    "$= nt_\\text{cache} + n\\text{MR}_\\text{cache}t_\\text{main memory}\n",
    "+ n\\text{MR}_\\text{cache}(1-\\text{MR}_\\text{main memory})t_\\text{memory} +  n\\text{MR}_\\text{cache}\\text{MR}_\\text{main memory}t_\\text{HDD}$ <br> \n",
    "$= nt_\\text{cache} + n\\text{MR}_\\text{cache}\\big(t_\\text{main memory}\n",
    "+ (1-\\text{MR}_\\text{main memory})t_\\text{memory} +  \\text{MR}_\\text{main memory}t_\\text{HDD}\\big)$ <br>\n",
    "$\\implies \\text{AMAT} = t_\\text{cache} + \\text{MR}_\\text{cache}\\big(t_\\text{main memory}\n",
    "+ (1-\\text{MR}_\\text{main memory})t_\\text{memory} +  \\text{MR}_\\text{main memory}t_\\text{HDD}\\big)$ <br>\n",
    "$\\implies \\text{AMAT} = 1 + 0.02\\big(100 + (0.999997)100 +  0.000003(10^6)\\big) = 5.059994$ cycles <br> \n",
    "\n",
    "After adding the TLB table: <br> \n",
    "\n",
    "Total running time = $ n \\cdot \\text{AMAT} $ <br>\n",
    "$= nt_\\text{cache} + n\\text{MR}_\\text{cache}t_\\text{TLB}\n",
    "+ n\\text{MR}_\\text{cache}(1-\\text{MR}_\\text{TLB})t_\\text{main memory} +  n\\text{MR}_\\text{cache}\\text{MR}_\\text{TLB}t_\\text{main memory} + \n",
    "n\\text{MR}_\\text{cache}\\text{MR}_\\text{TLB}\\text{MR}_\\text{main memory}t_\\text{HDD}\n",
    "$ <br> \n",
    "$= nt_\\text{cache} + n\\text{MR}_\\text{cache}\\big(t_\\text{TLB}\n",
    "+ (1-\\text{MR}_\\text{TLB})t_\\text{main memory} +  \\text{MR}_\\text{TLB}t_\\text{main memory} + \n",
    "\\text{MR}_\\text{TLB}\\text{MR}_\\text{main memory}t_\\text{HDD}\\big)\n",
    "$ <br> \n",
    "$\\implies \\text{AMAT} = t_\\text{cache} + \\text{MR}_\\text{cache}\\big(t_\\text{TLB}\n",
    "+ (1-\\text{MR}_\\text{TLB})t_\\text{main memory} +  \\text{MR}_\\text{TLB}t_\\text{main memory} + \n",
    "\\text{MR}_\\text{TLB}\\text{MR}_\\text{main memory}t_\\text{HDD}\\big)\n",
    "$ <br> \n",
    "$\\implies \\text{AMAT} = 1 + 0.02\\big(1\n",
    "+ (0.9995)100 +  (0.0005)100 + \n",
    "(0.0005)(0.000003)(10^6)\\big)$ = 3.02003 cycles\n",
    " <br> \n",
    "\n",
    "\n",
    "(b) <br> \n",
    "- Given 32-bit addressing is used with 4KB pages, that means $\\log_2\\big(2^{32} \\text{bytes} / (2^{12} \\text{bytes/page})\\big) = $20 bits from the 32 bits are used to address the physical page and the remaining 12 bits are used as page offset. \n",
    "- Given there is 8MB of physical memory, this mean $\\log_2\\big(2^{23} \\text{bytes} / (2^{12} \\text{bytes/page})\\big) = $ 11 bits are needed to locate the physical pages and the remaining 12 bits are used as page offset. \n",
    "- Consequently each TLB entry requires $20 + 11 + 1 = 32$ bits (the added bit is used for the valid flag). <br>\n",
    "TLB size = $64 \\text{ entries} \\cdot 32 \\text{bit/entry} = 2048$ bits  \n",
    "\n",
    "(c) <br> \n",
    "<img src=\"images\\P8_22.PNG\" />\n",
    "\n",
    "(d) <br> \n",
    "In a fully associated cache, there is no varying input since all the address/data combinations saved have to be read simultaneously. Hence the SRAM has $2^0 = 1$ row. In terms of width, the only SRAM row needs to store all the $2048$ bit. Consequently, the SRAM size is $1\\times2048$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-stanford",
   "metadata": {},
   "source": [
    "#### Exercise 8.23\n",
    "\n",
    "(a) <br> \n",
    "- Physical address have a size of 34 bits. Given page are 4KB ($2^{12}$ bytes), 31 - 12 = 19 bits are needed to address physical pages. \n",
    "- Virtual address have a size of 50 bits. Given page are 4KB ($2^{12}$ bytes), 50 - 12 = 38 bits are needed to address virtual pages. \n",
    "- TLB size is thus $128 \\text{ entry} \\times (19 + 38 + 1) $ bits = 7424 bits.  \n",
    "\n",
    "(b) <br> \n",
    "Illustration identical to 8.22 (c) with the following adjustements: <br>\n",
    "- Virtual page number size: 20 bits replaced with 38 bits \n",
    "- Physical page number size: 11 bits replaced with 19 bits\n",
    "- Multiplexer: 64 to 1 replaced with 128 to 1. \n",
    "\n",
    "\n",
    "(c) <br> \n",
    "The SRAM size is $1\\times7424$ (same logic used as in 8.22 part (d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-mouse",
   "metadata": {},
   "source": [
    "#### Exercise 8.24\n",
    "\n",
    "(a) <br> \n",
    "The TLB can be illustrated as part of the memory system, which replaces the `data/instruction` block of the original multicycle processor presented in chapter 7. <br>\n",
    "As shown below, TLB is not needed provided no cache miss occurs. However, if a cache miss does occur, the TLB is used to map the virtual address to the physical address. If the TLB does not miss, only one cycle is added to the read operation. If the TLB misses, two cycles are added to the read operating: one to access the page table, the othe to access the physical address.  The illustration below assumes there Main memory miss rate of 0%. \n",
    "<img src=\"images\\P8_24.PNG\" />\n",
    "\n",
    "(b) <br> \n",
    "Assuming TLB is sized such that one TLB read take 1 cycle, every instruction requiring a memory read or write will have their CPI increase by at least 1 to account for the added TLB read operation. If the TLB misses, the CPI of the instruction increases by 2. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-visitor",
   "metadata": {},
   "source": [
    "#### Exercise 8.25\n",
    "(a) <br> \n",
    "- 25 bits virtual addresses with 64KB pages ($2^{16}$ bytes) means virtual page address require $25 - 16 = 9$ bits. <br>\n",
    "- 22 bits physical addresses with 64KB pages ($2^{16}$ bytes) means virtual page address require $22 - 16 = 6$ bits. <br>\n",
    "- Each page table row requires space for the virtual page number (9 bit), physical page number (6 bit), valid bit (1 bit) and dirty bit (1 bit), giving a total of 9 + 6 + 1 + 1 = 17 bits. \n",
    "- The table page has a row for each virtual page. Hence the total size of the page table is $17 \\cdot 2^9 = 8704$ bits <br> \n",
    "\n",
    "(b) <br> \n",
    "- Reducing the page size from 16KB to 4KB will increase the page table size by a factor of 4.94 (4 in height and 21/17=1.235 in width). Since the page table is located in dedicated hardware, this means almost 5 times more SRAM memory will be needed to map the same amount of physical memory. \n",
    "- Additionally, in order for the TLB to maintain the same hit rate, it needs to cover the same fraction of the used virtual memory, meaning it has to map 4 times more table. Recall that TLB rely on fully associative memory, therefore 4 times more pages have to be compared. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-alfred",
   "metadata": {},
   "source": [
    "\n",
    "#### Exercise 8.26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-bargain",
   "metadata": {},
   "source": [
    "#### Exercise 8.27\n",
    "(a) <br> \n",
    "The maximum amount of virtual memory accessible by a single program depends on 1. how to operating systems allocate virtual memory between the programs that are running but also 2. how much virtual memory is available on the HDD or SSD. At most, a program could access all the available memory, (i.e. $2^{32}$ byte = 4GB) provided the HDD or SSD's capacity exceeds 4GB and it is the only program running.  \n",
    "\n",
    "(b) <br> \n",
    "The size of the hard drive does not significanly affect performance since the vast majority of memory read operations happen at the cache level, few of them happen at the physical memory (RAM) level and extremely few of them happen at the virtual memory level. \n",
    "\n",
    "(c) <br> \n",
    "Physical memory size affects performance by a larger amount than virtual memory size. However, the impact of physical memory size is especially noticeable when multiple programs are ran simultaneously or when large amounts of data are processed simultenously). Hence, provided the physical memory size is large enough for the application used, increments in its size will at most negligibly increase performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-percentage",
   "metadata": {},
   "source": [
    "#### Exercise 8.28\n",
    "#### Exercise 8.29\n",
    "#### Exercise 8.30"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
